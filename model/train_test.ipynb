{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "#from data import create_dir, download_data\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tarfile\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# setup directory structure for download\n",
    "\n",
    "#process_dir = Path(\"/Volumes/KINGSTON/Remote Sensing Presentation/Test\")\n",
    "\n",
    "#list(process_dir.glob(\".gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"batch_size\": 10, # make this bigger if you are not running on binder\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.0001,\n",
    "    \"device\": \"cpu\" # set to \"cuda\" if GPU is available\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in list(process_dir.glob(\"*.gz\")):\n",
    "    #tar = tarfile.open(x)\n",
    "    #tar.extractall(x.parent)\n",
    "    #tar.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cfa26adacb8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGlacierDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Custom Dataset for Glacier Data\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mIndexing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0mth\u001b[0m \u001b[0melement\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbinary\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GlacierDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Glacier Data\n",
    "\n",
    "    Indexing the i^th element returns the underlying image and the associated\n",
    "    binary y\n",
    "    \"\"\"\n",
    "    def __init__(self, x_paths, y_paths, imsize=512):\n",
    "        self.x_paths = x_paths\n",
    "        self.y_paths = y_paths\n",
    "        self.imsize = imsize\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_path = self.x_paths[index]\n",
    "        y_path = self.y_paths[index]\n",
    "        z = [np.load(p) for p in [x_path, y_path]]\n",
    "        z = [TF.to_tensor(z_).float() for z_ in z]\n",
    "\n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(z[0], output_size=(self.imsize, self.imsize))\n",
    "        z = [TF.crop(z_, i, j, h, w) for z_ in z]\n",
    "\n",
    "        # Random flipping\n",
    "        if random.random() > 0.5:\n",
    "            z = [TF.hflip(z_) for z_ in z]\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            z = [TF.vflip(z_) for z_ in z]\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "process_dir =Path(\"./Processed/Train\")\n",
    "paths = {\n",
    "    \"x\": list((process_dir).glob(\"x*\")),\n",
    "    \"y\": list((process_dir).glob(\"y*\"))\n",
    "}\n",
    "ds = GlacierDataset(paths[\"x\"], paths[\"y\"])\n",
    "loader = DataLoader(ds, batch_size=args[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from unet import Unet\n",
    "from train import train_epoch\n",
    "\n",
    "model = Unet(13, 3, 4, dropout=0.2).to(args[\"device\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train_epoch(model, loader, optimizer, args[\"device\"], epoch)\n",
    "    \n",
    "torch.save(model.state_dict(), data_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
